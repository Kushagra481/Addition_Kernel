{
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.11",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [],
      "dockerImageVersionId": 31041,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n",
        "!pip install transformers datasets evaluate accelerate matplotlib scikit-learn\n"
      ],
      "metadata": {
        "id": "rnQNwn5HIUce",
        "outputId": "cadad4ea-0c50-40b3-f36c-6089bd83b892",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://download.pytorch.org/whl/cu121\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (0.23.0+cu126)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.19.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torchvision) (2.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision) (11.3.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.56.2)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.12/dist-packages (4.0.0)\n",
            "Collecting evaluate\n",
            "  Downloading evaluate-0.4.6-py3-none-any.whl.metadata (9.5 kB)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.12/dist-packages (1.10.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (3.10.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.6.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.19.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.35.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.4)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.6.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from accelerate) (2.8.0+cu126)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (4.60.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.4.9)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (3.2.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (2.9.0.post0)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.16.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.12.15)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.1.10)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2025.8.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (3.4.0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.6.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=2.0.0->accelerate) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.3)\n",
            "Downloading evaluate-0.4.6-py3-none-any.whl (84 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: evaluate\n",
            "Successfully installed evaluate-0.4.6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile lmul_cuda_kernel.cu\n",
        "#include <torch/extension.h>\n",
        "#include <cuda_runtime.h>\n",
        "#include <vector>\n",
        "\n",
        "// L-mul offset function\n",
        "__device__ __forceinline__ int l_offset(int m) {\n",
        "    if (m <= 3) return m;\n",
        "    if (m == 4) return 3;\n",
        "    return 4;  // m > 4\n",
        "}\n",
        "\n",
        "// Standard matmul kernel\n",
        "__global__ void standard_matmul_kernel(const float* A, const float* B, float* C, int M, int N, int K) {\n",
        "    int row = blockIdx.y * blockDim.y + threadIdx.y;\n",
        "    int col = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "\n",
        "    if (row < M && col < N) {\n",
        "        float sum = 0.0f;\n",
        "        for (int k = 0; k < K; k++) {\n",
        "            sum += A[row * K + k] * B[k * N + col];\n",
        "        }\n",
        "        C[row * N + col] = sum;\n",
        "    }\n",
        "}\n",
        "\n",
        "// Launcher function for PyTorch\n",
        "void standard_matmul(torch::Tensor A, torch::Tensor B, torch::Tensor C) {\n",
        "    int M = A.size(0);\n",
        "    int K = A.size(1);\n",
        "    int N = B.size(1);\n",
        "\n",
        "    dim3 blockSize(16, 16);\n",
        "    dim3 gridSize((N + blockSize.x - 1)/blockSize.x, (M + blockSize.y - 1)/blockSize.y);\n",
        "\n",
        "    standard_matmul_kernel<<<gridSize, blockSize>>>(\n",
        "        A.data_ptr<float>(),\n",
        "        B.data_ptr<float>(),\n",
        "        C.data_ptr<float>(),\n",
        "        M, N, K\n",
        "    );\n",
        "}\n",
        "\n",
        "// Add your L-Mul kernels in a similar way\n",
        "// For example: lmul_addition_only, lmul_optimized_vectorized, lmul_integer_only\n",
        "// Each will have a __global__ kernel and a launcher taking torch::Tensor arguments\n",
        "\n",
        "PYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n",
        "    m.def(\"standard_matmul\", &standard_matmul, \"Standard matrix multiplication (CUDA)\");\n",
        "    // m.def(\"lmul_addition_only\", &lmul_addition_only_launcher, \"L-Mul addition-only\");\n",
        "    // m.def(\"lmul_optimized\", &lmul_optimized_launcher, \"L-Mul optimized vectorized\");\n",
        "    // m.def(\"lmul_integer_only\", &lmul_integer_only_launcher, \"L-Mul integer-only\");\n",
        "}\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-07-02T19:20:46.743581Z",
          "iopub.execute_input": "2025-07-02T19:20:46.744361Z",
          "iopub.status.idle": "2025-07-02T19:20:46.760202Z",
          "shell.execute_reply.started": "2025-07-02T19:20:46.744328Z",
          "shell.execute_reply": "2025-07-02T19:20:46.759451Z"
        },
        "id": "eBYoF49AEsGQ",
        "outputId": "e61f9014-b77c-4d28-a792-04d094e61eb2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting lmul_cuda_kernel.cu\n"
          ]
        }
      ],
      "execution_count": 14
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc -Xcompiler -fPIC -shared -o lmul_cuda_kernel.so lmul_cuda_kernel.cu -O3 --use_fast_math\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-07-02T19:21:14.253460Z",
          "iopub.execute_input": "2025-07-02T19:21:14.254094Z",
          "iopub.status.idle": "2025-07-02T19:21:18.088896Z",
          "shell.execute_reply.started": "2025-07-02T19:21:14.254066Z",
          "shell.execute_reply": "2025-07-02T19:21:18.088139Z"
        },
        "id": "Z3fkS7V0EsGU",
        "outputId": "93a2f3a7-2c03-4c7c-b302-0b341fb25797",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[01m\u001b[0m\u001b[01mlmul_cuda_kernel.cu(73)\u001b[0m: \u001b[01;35mwarning\u001b[0m #177-D: variable \u001b[01m\"sign_mult\"\u001b[0m was declared but never referenced\n",
            "              int sign_mult = sign_lut[idx];\n",
            "                  ^\n",
            "\n",
            "\u001b[01;36m\u001b[0m\u001b[01;36mRemark\u001b[0m: The warnings can be suppressed with \"-diag-suppress <warning-number>\"\n",
            "\n"
          ]
        }
      ],
      "execution_count": 5
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ninja\n"
      ],
      "metadata": {
        "id": "89RobLcwMzmb",
        "outputId": "397e9319-cd48-4c53-98eb-19601974f2bf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ninja\n",
            "  Downloading ninja-1.13.0-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (5.1 kB)\n",
            "Downloading ninja-1.13.0-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (180 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/180.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m180.7/180.7 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: ninja\n",
            "Successfully installed ninja-1.13.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf /root/.cache/torch_extensions/*\n"
      ],
      "metadata": {
        "id": "0X33T6wsNGft"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Install ninja (needed for PyTorch extensions)\n",
        "!pip install ninja --quiet\n",
        "\n",
        "# Step 2: Clean previous builds (optional but recommended)\n",
        "!rm -rf /root/.cache/torch_extensions/*\n",
        "\n",
        "# Step 3: Compile & load the CUDA extension\n",
        "from torch.utils.cpp_extension import load\n",
        "\n",
        "lmul_cuda = load(\n",
        "    name=\"lmul_cuda\",\n",
        "    sources=[\"/content/lmul_cuda_kernel.cu\"],  # full path\n",
        "    extra_cuda_cflags=[\"-O3\", \"--use_fast_math\"],\n",
        "    verbose=True  # prints compilation logs\n",
        ")\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "C139aCF9EsGU",
        "outputId": "47c21a18-f89b-4a4e-895e-9bfed6da6cd8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "W1007 19:48:35.369000 747 torch/utils/cpp_extension.py:2425] TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. \n",
            "W1007 19:48:35.369000 747 torch/utils/cpp_extension.py:2425] If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'] to specific architectures.\n"
          ]
        }
      ],
      "execution_count": 16
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# Define matrix sizes\n",
        "M, N, K = 512, 512, 512  # example sizes\n",
        "\n",
        "# Create input matrices on GPU\n",
        "A = torch.randn(M, K, device='cuda')\n",
        "B = torch.randn(K, N, device='cuda')\n",
        "\n",
        "# Output matrix\n",
        "C = torch.zeros(M, N, device='cuda')\n"
      ],
      "metadata": {
        "id": "1F3AAXatMtev"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lmul_cuda.standard_matmul(A, B, C)\n"
      ],
      "metadata": {
        "id": "zE_N0AflOS1T"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import time\n",
        "\n",
        "# Matrix dimensions\n",
        "M, N, K = 512, 512, 512\n",
        "\n",
        "# Random input tensors on GPU\n",
        "A = torch.randn(M, K, device='cuda')\n",
        "B = torch.randn(K, N, device='cuda')\n",
        "\n",
        "# Output tensors\n",
        "C_standard = torch.zeros(M, N, device='cuda')\n",
        "C_lmul = torch.zeros(M, N, device='cuda')\n",
        "\n",
        "# Block and grid sizes\n",
        "block = (16, 16)\n",
        "grid = ((N + block[0] - 1)//block[0], (M + block[1] - 1)//block[1])"
      ],
      "metadata": {
        "id": "nvEn2nyxOZNi"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.synchronize()\n",
        "start = time.time()\n",
        "C_standard = A @ B\n",
        "torch.cuda.synchronize()\n",
        "time_standard = time.time() - start\n",
        "print(f\"PyTorch matmul time: {time_standard*1000:.3f} ms\")\n"
      ],
      "metadata": {
        "id": "mC5SW1loOpmA",
        "outputId": "bcd6086d-56f7-4650-abf4-084601e52af1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PyTorch matmul time: 135.921 ms\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import cupy as cp\n",
        "import torch\n",
        "import time\n",
        "\n",
        "# Convert PyTorch tensors to CuPy arrays\n",
        "A_cp = cp.asarray(A)\n",
        "B_cp = cp.asarray(B)\n",
        "C_cp = cp.zeros((M, N), dtype=cp.float32)\n",
        "\n",
        "kernel_code = r'''\n",
        "extern \"C\" __global__\n",
        "void lmul_addition_only(float* A, float* B, float* C, int M, int N, int K) {\n",
        "    int row = blockIdx.y * blockDim.y + threadIdx.y;\n",
        "    int col = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "\n",
        "    if (row < M && col < N) {\n",
        "        float sum = 0.0f;\n",
        "        for (int k = 0; k < K; k++) {\n",
        "            float a = A[row*K + k];\n",
        "            float b = B[k*N + col];\n",
        "            sum += a + b;  // simplified addition-only\n",
        "        }\n",
        "        C[row*N + col] = sum;\n",
        "    }\n",
        "}\n",
        "'''\n",
        "\n",
        "# Compile\n",
        "module = cp.RawKernel(kernel_code, 'lmul_addition_only')\n",
        "\n",
        "# Launch\n",
        "block = (16, 16)\n",
        "grid = ((N + block[0]-1)//block[0], (M + block[1]-1)//block[1])\n",
        "start = time.time()\n",
        "module(grid, block, (A_cp, B_cp, C_cp, M, N, K))\n",
        "cp.cuda.Stream.null.synchronize()\n",
        "print(\"L-Mul time:\", (time.time()-start)*1000, \"ms\")\n",
        "\n",
        "# Convert back to PyTorch\n",
        "C_lmul = torch.as_tensor(C_cp)\n"
      ],
      "metadata": {
        "id": "OLiwAg8-OrBf",
        "outputId": "8b089add-25d6-43dc-a82f-878de7ae0f06",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "L-Mul time: 246.0620403289795 ms\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Max difference:\", (C_standard - C_lmul).abs().max().item())\n"
      ],
      "metadata": {
        "id": "s_rX2QfNO1pm",
        "outputId": "8fd703ff-e4c1-4e9e-b86b-1bd3d88cf911",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Max difference: 198.2362060546875\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import time\n",
        "\n",
        "# Example: standard matmul\n",
        "M, K, N = 1024, 1024, 1024\n",
        "A = torch.randn(M, K, device='cuda')\n",
        "B = torch.randn(K, N, device='cuda')\n",
        "\n",
        "# Measure PyTorch matmul\n",
        "torch.cuda.synchronize()\n",
        "start = time.time()\n",
        "C_std = torch.matmul(A, B)\n",
        "torch.cuda.synchronize()\n",
        "time_std = time.time() - start  # seconds\n",
        "power_gpu = 70  # watts\n",
        "energy_std = power_gpu * time_std  # joules\n",
        "print(f\"Standard matmul: {time_std*1000:.2f} ms, Energy ~ {energy_std:.2f} J\")\n",
        "\n",
        "# L-Mul (addition-only) via CuPy\n",
        "import cupy as cp\n",
        "A_cp = cp.asarray(A)\n",
        "B_cp = cp.asarray(B)\n",
        "C_cp = cp.zeros((M, N), dtype=cp.float32)\n",
        "\n",
        "kernel_code = r'''\n",
        "extern \"C\" __global__\n",
        "void lmul_addition_only(float* A, float* B, float* C, int M, int N, int K){\n",
        "    int row = blockIdx.y * blockDim.y + threadIdx.y;\n",
        "    int col = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "    if (row < M && col < N){\n",
        "        float sum = 0.0f;\n",
        "        for (int k=0;k<K;k++){\n",
        "            sum += A[row*K+k] + B[k*N+col];  // addition-only\n",
        "        }\n",
        "        C[row*N+col] = sum;\n",
        "    }\n",
        "}\n",
        "'''\n",
        "\n",
        "module = cp.RawKernel(kernel_code, 'lmul_addition_only')\n",
        "block = (16, 16)\n",
        "grid = ((N + 15)//16, (M + 15)//16)\n",
        "\n",
        "start = time.time()\n",
        "module(grid, block, (A_cp, B_cp, C_cp, M, N, K))\n",
        "cp.cuda.Stream.null.synchronize()\n",
        "time_lmul = time.time() - start\n",
        "energy_lmul = power_gpu * time_lmul\n",
        "\n",
        "print(f\"L-Mul kernel: {time_lmul*1000:.2f} ms, Energy ~ {energy_lmul:.2f} J\")\n"
      ],
      "metadata": {
        "id": "0K7-8ouwPC3e",
        "outputId": "dd43c934-98cb-4fa4-eff1-1d1a0ae01dd5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Standard matmul: 1.78 ms, Energy ~ 0.12 J\n",
            "L-Mul kernel: 61.55 ms, Energy ~ 4.31 J\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wG9_SzUrPKyc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}